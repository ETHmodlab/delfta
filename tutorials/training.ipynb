{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example training script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial shows a brief example of how the networks were trained. The specific example below is for $\\Delta$-learning of formation energy in a single-task setting. We'll start with the imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import os\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from delfta.net import EGNN\n",
    "from delfta.net_utils import MODEL_HPARAMS\n",
    "from delfta.utils import DATA_PATH, ROOT_PATH\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.utils.undirected import to_undirected\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll download the training data if that hasn't happened yet (not done by default during the setup of `delfta`, since the original training files aren't needed to run the trained models). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "if not os.path.exists(os.path.join(DATA_PATH, \"qmugs\", \"qmugs_conf00.h5\")): \n",
    "    from delfta.download import DATASET_REMOTE, download\n",
    "    import tarfile \n",
    "    \n",
    "    download(DATASET_REMOTE, os.path.join(DATA_PATH, \"qmugs.tar.gz\"))\n",
    "    with tarfile.open(os.path.join(DATA_PATH, \"qmugs.tar.gz\")) as handle:\n",
    "        handle.extractall(DATA_PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll define a new dataset class which is similar to the one in `delfta.net_utils`, but it doesn't load all the molecules in memory - which is better for training with a large number of molecules."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class DatasetSingletaskh5(Dataset):\n",
    "    def __init__(\n",
    "        self, txtfile, prop,\n",
    "    ):\n",
    "\n",
    "        # read txt\n",
    "        with open(txtfile, \"r\") as f:\n",
    "            chembls = [elem.rstrip(\"\\n\") for elem in f.readlines()]\n",
    "\n",
    "        # create dict on the fly: idx -> chembl\n",
    "        nums = list(range(0, len(chembls)))\n",
    "        self.idx2chembl = {}\n",
    "        for x in range(len(chembls)):\n",
    "            dict = {nums[x]: chembls[x]}\n",
    "            self.idx2chembl.update(dict)\n",
    "\n",
    "        # read h5\n",
    "        self.h5f0 = h5py.File(os.path.join(DATA_PATH, \"qmugs\", \"qmugs_conf00.h5\"), \"r\")\n",
    "        self.h5f1 = h5py.File(os.path.join(DATA_PATH, \"qmugs\", \"qmugs_conf01.h5\"), \"r\")\n",
    "        self.h5f2 = h5py.File(os.path.join(DATA_PATH, \"qmugs\", \"qmugs_conf02.h5\"), \"r\")\n",
    "\n",
    "        # define property of interest\n",
    "        self.prop = prop\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        chembl_id = self.idx2chembl[idx]\n",
    "\n",
    "        #### nodes coordinates and target\n",
    "        if \"conf_00\" in chembl_id:\n",
    "            atomids = torch.LongTensor(self.h5f0[str(chembl_id)][\"atomids\"])\n",
    "            coords = torch.FloatTensor(self.h5f0[str(chembl_id)][\"coords\"])\n",
    "            target = torch.FloatTensor(self.h5f0[str(chembl_id)][self.prop])\n",
    "        elif \"conf_01\" in chembl_id:\n",
    "            atomids = torch.LongTensor(self.h5f1[str(chembl_id)][\"atomids\"])\n",
    "            coords = torch.FloatTensor(self.h5f1[str(chembl_id)][\"coords\"])\n",
    "            target = torch.FloatTensor(self.h5f1[str(chembl_id)][self.prop])\n",
    "        elif \"conf_02\" in chembl_id:\n",
    "            atomids = torch.LongTensor(self.h5f2[str(chembl_id)][\"atomids\"])\n",
    "            coords = torch.FloatTensor(self.h5f2[str(chembl_id)][\"coords\"])\n",
    "            target = torch.FloatTensor(self.h5f2[str(chembl_id)][self.prop])\n",
    "\n",
    "        #### edges\n",
    "        edge_index = np.array(nx.complete_graph(atomids.size(0)).edges())\n",
    "        edge_index = to_undirected(torch.from_numpy(edge_index).t().contiguous())\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=coords.shape[0])\n",
    "\n",
    "        #### graph object\n",
    "        graph_data = Data(\n",
    "            atomids=atomids,\n",
    "            coords=coords,\n",
    "            edge_index=edge_index,\n",
    "            target=target,\n",
    "            num_nodes=atomids.size(0),\n",
    "        )\n",
    "\n",
    "        return graph_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2chembl)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll define the training and evaluation loops:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mae_loss = lambda x, y: F.l1_loss(x, y).item()\n",
    "\n",
    "\n",
    "def train_loop(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    training_loss = []\n",
    "\n",
    "    for g_batch in tqdm(loader, total=len(loader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        g_batch = g_batch.to(DEVICE)\n",
    "        target = g_batch.target\n",
    "\n",
    "        prediction = model(g_batch).squeeze(1)\n",
    "\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mae = mae_loss(prediction, target)\n",
    "            training_loss.append(mae)\n",
    "\n",
    "    return np.mean(training_loss), np.std(training_loss)\n",
    "\n",
    "\n",
    "def eval_loop(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    maes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for g_batch in tqdm(loader, total=len(loader)):\n",
    "            g_batch = g_batch.to(DEVICE)\n",
    "            prediction = model(g_batch).squeeze(1)\n",
    "            maes.append(mae_loss(prediction, g_batch.target))\n",
    "\n",
    "    eval_mae = sum(maes) / len(maes)\n",
    "    return eval_mae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally we can start training the model:  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "save_path = os.path.join(ROOT_PATH, \"tutorials\", \"training_evaluation\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "train_path = os.path.join(DATA_PATH, \"qmugs\", \"train_example.txt\") # this includes only 50 molecules so that in this tutorial, we quickly see results\n",
    "eval_path = os.path.join(DATA_PATH, \"qmugs\", \"eval_example.txt\")  \n",
    "prop = \"DELTA_ENERGY\"\n",
    "model_param = MODEL_HPARAMS[\"single_energy_delta\"]\n",
    "\n",
    "train_data = DatasetSingletaskh5(txtfile=train_path, prop=prop)\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "validation_data = DatasetSingletaskh5(txtfile=eval_path, prop=prop)\n",
    "validation_loader = DataLoader(\n",
    "    validation_data, batch_size=2, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = EGNN(\n",
    "    n_outputs=model_param.n_outputs,\n",
    "    global_prop=model_param.global_prop,\n",
    "    n_kernels=model_param.n_kernels,\n",
    "    mlp_dim=model_param.mlp_dim,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-10)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.7, patience=20, verbose=True\n",
    ")\n",
    "\n",
    "train_m_losses, train_std_losses = [], []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "epoch_maes = []\n",
    "min_mae = 1e10\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}...\", flush=True)\n",
    "\n",
    "    m_loss, std_loss = train_loop(model, train_loader, optimizer, nn.MSELoss())\n",
    "    train_m_losses.append(m_loss)\n",
    "    train_std_losses.append(std_loss)\n",
    "\n",
    "    eval_mae = eval_loop(model, validation_loader)\n",
    "    val_losses.append(eval_mae)\n",
    "    scheduler.step(eval_mae)\n",
    "\n",
    "    if eval_mae < min_mae:\n",
    "\n",
    "        min_mae = eval_mae\n",
    "        val_maes.append(eval_mae)\n",
    "        epoch_maes.append(epoch)\n",
    "        print(f\"New min eval_mae in epoch {epoch}: {eval_mae:.6f}\", flush=True)\n",
    "        torch.save(\n",
    "            model.state_dict(), os.path.join(save_path, \"model.pt\"),\n",
    "        )\n",
    "        torch.save(\n",
    "            [train_m_losses, train_std_losses, val_losses],\n",
    "            os.path.join(save_path, \"loss_train_eval.pt\"),\n",
    "        )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 25/25 [00:46<00:00,  1.88s/it]\n",
      "100%|██████████| 25/25 [00:13<00:00,  1.84it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New min eval_mae in epoch 0: 0.931423\n",
      "Epoch 2/100...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      " 56%|█████▌    | 14/25 [00:20<00:12,  1.10s/it]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('delfta': conda)"
  },
  "interpreter": {
   "hash": "98cf6d433ae36a73117aebd7a9d2aa2f181e76ebc480da2102b87173d53f82a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}